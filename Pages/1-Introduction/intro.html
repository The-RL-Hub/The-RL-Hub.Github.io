<!DOCTYPE html>
<html lang="fa" dir="rtl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RLH: Introduction</title>
    <link rel="stylesheet" href="styles.css">
    <script src="script.js" defer></script>
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Vazirmatn:wght@400;600&display=swap" rel="stylesheet">
    <!-- Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>
<body>
    <header>
        <div class="logo">
            <img src="Pictures/logo.png" alt="Logo">
            <span>مقدمه</span>
        </div>
        <nav>
            <a href="https://the-rl-hub.github.io/">خانه</a>
            <a href="#">درباره ما</a>
            <a href="#"></a>
        </nav>
    </header>

    <main>
        <aside class="sidebar">
            <ul>
                <li><a href="#trial-error">یادگیری از طریق آزمون و خطا</a></li>
                <li><a href="#rl-overview">Reinforcement Learning چیه؟</a></li>
                <li><a href="#rl-vs-supervised">RL و تفاوت با Supervised و Unsupervised Learning</a></li>
                <li><a href="#rl-examples">مثال‌هایی از RL در عمل</a></li>
                <li><a href="#rl-elements">المنت های RL</a></li>
            </ul>
        </aside>
    
        <section class="content">
            <h1>مقدمه‌ای بر Reinforcement Learning</h1>
            <hr>
    
            <h2 id="trial-error">یادگیری از طریق آزمون و خطا: شهود اصلی در Reinforcement Learning</h2>
    
            <p>
                Reinforcement Learning یا RL رو می‌شه به‌عنوان نمونه‌ی کامل یادگیری از طریق انجام دادن در نظر گرفت. فرض کن یه بچه بخواد دوچرخه‌سواری یاد بگیره. اولش ممکنه تلوتلو بخوره، بیفته یا برای حفظ تعادل کلی تقلا کنه. اما با بارها و بارها تلاش کردن—آزمون و خطا—کم‌کم می‌فهمه چی جواب می‌ده و چی نمی‌ده. هر بار افتادن بهش یاد می‌ده که چطوری تعادلشو حفظ کنه و هر حرکت موفق هم همون کار درستو تو ذهنش تثبیت می‌کنه. این دقیقاً شبیه چیزی هست که تو RL اتفاق می‌افته: یادگیری از طریق آزمون و خطا.
            </p>

            
            <div class="image-container">
                <img src="Pictures/1.jpg" alt="A Robot" style="width: 50%; height: auto;">
                <p class="image-caption">شکل 1: تصویری از برگرفته از کورس CS188 دانشگاه برکلی<p>
            </div>
    
            <div class="box box-question">
                <strong>سؤال: فکر کن به یه چیزی که تازگیا یاد گرفتی، مثلاً آشپزی کردن یا نواختن یه ساز. به نظرت تو اولین تلاش موفق بودی یا با نتایج کارت تغییرش دادی؟ این موضوع چه شباهتی به چیزی داره که RL انجام می‌ده؟</strong> 
            </div>
            <p>
                ایده اصلی RL ساده اما خیلی قویه: ایجنتا با تعامل با محیطشون یاد می‌گیرن و با توجه به بازخورد (پاداش) که می‌گیرن، کاراشونو تنظیم می‌کنن. ایجنت گزینه‌های مختلفو بررسی می‌کنه، چیزایی که قبلاً امتحان نکرده رو تست می‌کنه و بین استفاده از چیزایی که می‌دونه خوب جواب می‌ده (بهره‌برداری) و امتحان کردن گزینه‌های جدید که شاید بهتر باشن (کاوش) یه تعادل برقرار می‌کنه.
            </p>

            <div class="box box-question">
                <strong>یه فکر: فرض کن یه دستگاه اسلات(از این دستگاها که تو فیلما میبینیم) داری که چندتا اهرم داره و هرکدومشون پاداشای متفاوت اما ناشناخته‌ای دارن. چطوری تصمیم می‌گیری کدوم اهرمو بکشی؟ به همون اهرمی که آخرین بار بهت جایزه داده بچسبی یا یه اهرم جدید امتحان کنی؟ این دقیقاً همون مفهوم exploration exploitation dilemma تو RL هست.</strong> 
            </div>

            <p>
                تعادلی که به اسم exploration-exploitation trade-off شناخته می‌شه، یکی از چالشای اصلی تو RL به حساب میاد. مثلاً یه ربات که می‌خواد تو یه هزارتو راه پیدا کنه باید تصمیم بگیره که آیا از مسیری که می‌دونه به هدف می‌رسه استفاده کنه (بهره‌برداری) یا یه مسیر جدید امتحان کنه که شاید کوتاه‌تر باشه (کاوش). پیدا کردن این تعادل خیلی مهمه، چون اگه زیادی کاوش کنه وقتو تلف می‌کنه و اگه فقط بهره‌برداری کنه شاید تو رفتارای غیربهینه گیر کنه.
            </p>
            
    
            <hr>
    
            <h2 id="rl-overview">Reinforcement Learning چیه؟ یه مسئله، یه راه‌حل و یه حوزه تحقیقاتی</h2>
    
            <p>
                Reinforcement Learning از اون چیزاییه که خیلی انعطاف‌پذیره و می‌تونه تو سه نقش مختلف ظاهر بشه:
            </p>
    
            <ul>
                <li><strong>یه مسئله:</strong> تو هسته خودش، RL مسئله یادگیری هدفمند تو محیطای نامطمئن رو بررسی می‌کنه. مثلاً یه ربات چطوری باید تصمیم بگیره که چه کاری انجام بده تا هم باتریشو حفظ کنه و هم کارشو تموم کنه؟ یا یه برنامه شطرنج چطور باید حرکتای خودشو انتخاب کنه که برنده بشه؟</li>
                <li><strong>یه راه‌حل:</strong> RL شامل یه سری روشای حل مثل Q-learning، policy gradients یا deep reinforcement learning هست که برای حل این مسئله طراحی شدن. این روشا به ایجنت کمک می‌کنن که از بازخورد یاد بگیره و رفته‌رفته بهتر بشه.</li>
                <li><strong>یه حوزه ریسرچ:</strong> علاوه بر مسائل و راه‌حل‌ها، RL یه حوزه تحقیقاتی حسابی پر و پیمونه که ریاضیات، علوم کامپیوتر، علوم اعصاب و روانشناسی رو شامل می‌شه. این حوزه بررسی می‌کنه که ایجنت‌ها چطوری یاد می‌گیرن و چطوری می‌شه این یادگیریو کارآمدتر و قابل اعمال‌تر کرد.</li>
            </ul>

            <p>
                یه نکته خیلی مهم تو RL، تمایز بین مسائل و روشای حل هست. مثلاً، exploration exploitation dilemma یه مسئله مشترک تو خیلی از کارای RL هست، اما می‌شه با روشای مختلفی مثل استراتژی‌های ε-greedy یا Thompson sampling حلش کرد. اگه این تمایزو قاطی کنیم، خیلی چیزا اشتباه می‌شه، چون ممکنه یه مسئله بسته به شرایط راه‌حلای کاملاً متفاوتی بخواد.
            </p>

            <div class="box box-tip">
                <strong>کاوش بیشتر:</strong> استراتژی‌های ε-greedy رو یه نگاهی بنداز یا ببین "exploration" تو ویدیوگیم موردعلاقت چطوری انجام می‌شه. طراحای بازی چطوری این تعادلو برقرار می‌کنن؟
            </div>


    
            <hr>
    
            <h2 id="rl-vs-supervised">RL چه فرقی با Supervised و Unsupervised Learning داره؟</h2>
    
            <p>
                Reinforcement Learning معمولاً با بقیه پارادایم‌های یادگیری ماشین مثل <strong>supervised</strong> و <strong>unsupervised learning</strong> مقایسه می‌شه، ولی از اساس یه چیز کاملاً متفاوتیه.
            </p>
            <ul>
                <li><strong>Supervised Learning: </strong>تو یادگیری تحت نظارت (supervised)، ایجنت از یه مجموعه داده برچسب‌دار که یه سرپرست آماده کرده، یاد می‌گیره. مثلاً یه سیستم تشخیص گربه و سگ با کلی عکس که به عنوان "گربه" یا "سگ" برچسب خوردن آموزش می‌بینه. هدف اینه که ایجنت از این نمونه‌ها تعمیم بده و عکسای جدیدو درست دسته‌بندی کنه.

                    <div class="box box-question">
                        <strong>یه سؤال:</strong> اگه supervised شبیه یادگیری با یه معلم باشه، RL شبیه چیه؟ می‌شه گفت شبیه یادگیری از طریق کشف و تجربه است؟
                    </div>
                    ولی RL به داده‌های برچسب‌دار نیاز نداره. در عوض، مستقیماً از تعامل با محیط یاد می‌گیره. هیچ جواب مشخص و از پیش تعیین‌شده‌ای وجود نداره؛ ایجنت باید خودش کشفش کنه. مثلاً رباتی که یاد می‌گیره راه بره، دستورالعمل خاصی نمی‌گیره که چطوری پاهاشو حرکت بده—بلکه با امتحان کردن و دیدن نتیجه، خودش یاد می‌گیره.
                </li><br>
                <li><strong>Unsupervised Learning: </strong>تو یادگیری بدون نظارت (unsupervised)، هدف کشف الگوها یا ساختارهای پنهان تو داده‌هاست، مثل خوشه‌بندی مشتریا یا کاهش ابعاد داده‌ها. RL یه شباهتی به این روش داره چون به داده برچسب‌دار نیاز نداره، ولی تمرکزش بیشتر روی به حداکثر رسوندن سیگنال پاداش هست، نه کشف الگوها. مثلاً یه ایجنت RL که شطرنج بازی می‌کنه دنبال خوشه‌بندی حرکات شطرنج نیست—فقط می‌خواد بازیو ببره.
                    <div class="box box-question">
                        <strong>یه سؤال:</strong> چرا فکر می‌کنی RL بیشتر شبیه به آزمون و خطاست تا یادگیری تحت نظارت یا بدون نظارت؟ RL چه مسائلیو بهتر می‌تونه حل کنه؟
                    </div>
                    به طور کلی، RL یه موقعیت منحصربه‌فرد داره و به‌عنوان یه پارادایم سوم تو یادگیری ماشین شناخته می‌شه. RL کاوش، تعامل و رفتار هدف‌محوری رو ترکیب می‌کنه که تو بقیه پارادایما نیست، و برای حل مسائلی که راهنمایی مستقیم توشون وجود نداره، خیلی قویه.
                </li>
            </ul>
    
            
    
            <hr>
    
            <h2 id="rl-examples">مثال‌هایی از RL در عمل</h2>
    
            <ul>
                <li><strong>ماشین‌های خودران:</strong>این ماشینا از RL استفاده می‌کنن تا تو جاده حرکت کنن، از موانع رد شن و تصمیم‌گیریای لحظه‌ای کنن. ایجنت (ماشین) با محیط (شرایط جاده و ترافیک) تعامل داره و یاد می‌گیره ایمنی و کارایی رو به حداکثر برسونه و تصادفاتو کم کنه.
                    <div class="box box-tip">
                        <strong>یه مساله واقعی:</strong> تصور کن یه سیستم RL برای ماشینای خودران طراحی می‌کنی. چطوری به ماشین پاداش می‌دی؟ فقط اجتناب از تصادف کافیه یا برای مصرف انرژی کمتر یا راحتی مسافرا هم پاداش می‌ذاری؟
                    </div>
                </li>
                <li><strong>کنترل رباتیک:</strong> رباتایی که تو کارخونه قطعاتو مونتاژ می‌کنن، با استفاده از RL حرکات خودشونو بهبود می‌دن و دقتو بالا می‌برن و مصرف انرژی رو کم می‌کنن.</li>
                <li><strong>بازی‌ها:</strong> از AlphaGo که تو بازی Go استاد شد گرفته تا ایجنتای RL که تو بازیای پیچیده ویدیویی مهارت پیدا کردن، RL پتانسیلشو برای حل مسائل سخت با یادگیری مستقیم از تجربه نشون داده.</li>
            </ul>


            <hr>

            <h2 id="rl-elements">المنت های RL</h2>
            <p>
                Reinforcement Learning بدیهتاً چندتا المنت اصلی داره و این المنتا تعیین می‌کنن ایجنت چجوری یاد بگیره، خودش رو وفق بده و تو یه محیط رفتار کنه. این عناصر شامل Policy، Reward Signal، Value Function و به‌صورت اختیاری Model ای برای Environment هستن. هرکدوم از اینا یه نقش مشخص دارن که باعث می‌شن ایجنت درست کار کنه و بهتر یاد بگیره.
            </p>

            <div class="image-container">
                <img src="Pictures/2.png" alt="full scheme" style="width: 40%; height: auto;">
                <p class="image-caption">شکل 2: چرخه RL و چارچوب تعامل Agent و Environment<p>
            </div>


            <h3>Value Function</h3>
            <p>
                Value Function نشون می‌ده چقدر خوبه که تو یه state خاص باشیم، با در نظر گرفتن پاداش‌های فوری و آینده. این تابع به ایجنت کمک می‌کنه روی کارهایی تمرکز کنه که شاید الان سودی نداشته باشن، ولی تو بلندمدت مفید باشن.
            </p>

            <h4>ویژگی‌های Value Function</h4>
            <ul>
                <li><strong>نگاه بلندمدت:</strong> Value پاداش‌های جمع‌شده تو طول زمان رو در نظر می‌گیره، نه فقط پاداش بعدی رو.</li>
                <li><strong>پویا بودن:</strong> Value‌ها با گذر زمان و تجربه ایجنت به‌روز می‌شن.</li>
                <li><strong>راهنمای بهتر شدن Policy:</strong> ایجنت از Value Function برای تصمیم‌گیری‌های بهتر استفاده می‌کنه.</li>
            </ul>

            <div class="box box-question">
                <strong>یه سؤال:</strong> تو زندگی، پیش میاد که منافع بلندمدت رو به رضایت کوتاه‌مدت ترجیح بدی؟ این شبیه نحوه استفاده ایجنت از Value Function نیست؟
            </div>

            <div class="box box-tip">
                <strong>شهود:</strong> فرض کن داری تو یه هزارتو راه می‌ری. حتی اگه یه مسیر مستقیم به خروجی نرسه (پاداش فوری)، ممکنه تو رو به نقطه‌ای نزدیک‌تر به خروجی برسونه (پاداش‌های آینده). ارزش بودن تو یه نقطه خاص از هزارتو به این بستگی داره که چقدر <strong>امیدوارکننده‌ست</strong> برای رسیدن به خروجی.
            </div>
            
            

            <div class="next-section">
                <a href="#next-section" class="button">بخش بعدی</a>
            </div>
    
        </section>
    </main>
    

    <footer class="social-footer">
        <a href="https://t.me/RL_Hub" target="_blank"><i class="fab fa-telegram"></i></a>
        <a href="https://github.com/The-RL-Hub" target="_blank"><i class="fab fa-github"></i></a>
        <a href="mailto:arshiyagharoony@gmail.com"><i class="fas fa-envelope"></i></a>
    </footer>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
    <script>
        hljs.highlightAll();
    </script>
    <script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
    
