<!DOCTYPE html>
<html lang="fa" dir="rtl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RLH: Introduction</title>
    <link rel="stylesheet" href="styles.css">
    <script src="script.js" defer></script>
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Vazirmatn:wght@400;600&display=swap" rel="stylesheet">
    <!-- Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>
<body>
    <header>
        <div class="logo">
            <img src="Pictures/logo.png" alt="Logo">
            <span>مقدمه</span>
        </div>
        <nav>
            <a href="https://the-rl-hub.github.io/">خانه</a>
            <a href="#">درباره ما</a>
            <a href="#"></a>
        </nav>
    </header>

    <main>
        <aside class="sidebar">
            <ul>
                <li><a href="#trial-error">یادگیری از طریق آزمون و خطا</a></li>
                <li><a href="#rl-overview">Reinforcement Learning چیه؟</a></li>
                <li><a href="#rl-vs-supervised">RL و تفاوت با Supervised و Unsupervised Learning</a></li>
                <li><a href="#rl-examples">مثال‌هایی از RL در عمل</a></li>
                <li><a href="#rl-elements">المنت های RL</a></li>
            </ul>
        </aside>
    
        <section class="content">
            <h1>مقدمه‌ای بر Reinforcement Learning</h1>
            <hr>
    
            <h2 id="trial-error">یادگیری از طریق آزمون و خطا: شهود اصلی در Reinforcement Learning</h2>
    
            <p>
                Reinforcement Learning یا RL رو می‌شه به‌عنوان نمونه‌ی کامل یادگیری از طریق انجام دادن در نظر گرفت. فرض کن یه بچه بخواد دوچرخه‌سواری یاد بگیره. اولش ممکنه تلوتلو بخوره، بیفته یا برای حفظ تعادل کلی تقلا کنه. اما با بارها و بارها تلاش کردن—آزمون و خطا—کم‌کم می‌فهمه چی جواب می‌ده و چی نمی‌ده. هر بار افتادن بهش یاد می‌ده که چطوری تعادلشو حفظ کنه و هر حرکت موفق هم همون کار درستو تو ذهنش تثبیت می‌کنه. این دقیقاً شبیه چیزی هست که تو RL اتفاق می‌افته: یادگیری از طریق آزمون و خطا.
            </p>

            
            <div class="image-container">
                <img src="Pictures/1.jpg" alt="A Robot" style="width: 50%; height: auto;">
                <p class="image-caption">شکل 1: تصویری از برگرفته از کورس CS188 دانشگاه برکلی<p>
            </div>
    
            <div class="box box-question">
                <strong>سؤال: فکر کن به یه چیزی که تازگیا یاد گرفتی، مثلاً آشپزی کردن یا نواختن یه ساز. به نظرت تو اولین تلاش موفق بودی یا با نتایج کارت تغییرش دادی؟ این موضوع چه شباهتی به چیزی داره که RL انجام می‌ده؟</strong> 
            </div>
            <p>
                ایده اصلی RL ساده اما خیلی قویه: ایجنتا با تعامل با محیطشون یاد می‌گیرن و با توجه به بازخورد (پاداش) که می‌گیرن، کاراشونو تنظیم می‌کنن. ایجنت گزینه‌های مختلفو بررسی می‌کنه، چیزایی که قبلاً امتحان نکرده رو تست می‌کنه و بین استفاده از چیزایی که می‌دونه خوب جواب می‌ده (بهره‌برداری) و امتحان کردن گزینه‌های جدید که شاید بهتر باشن (کاوش) یه تعادل برقرار می‌کنه.
            </p>

            <div class="box box-question">
                <strong>یه فکر: فرض کن یه دستگاه اسلات(از این دستگاها که تو فیلما میبینیم) داری که چندتا اهرم داره و هرکدومشون پاداشای متفاوت اما ناشناخته‌ای دارن. چطوری تصمیم می‌گیری کدوم اهرمو بکشی؟ به همون اهرمی که آخرین بار بهت جایزه داده بچسبی یا یه اهرم جدید امتحان کنی؟ این دقیقاً همون مفهوم exploration exploitation dilemma تو RL هست.</strong> 
            </div>

            <p>
                تعادلی که به اسم exploration-exploitation trade-off شناخته می‌شه، یکی از چالشای اصلی تو RL به حساب میاد. مثلاً یه ربات که می‌خواد تو یه هزارتو راه پیدا کنه باید تصمیم بگیره که آیا از مسیری که می‌دونه به هدف می‌رسه استفاده کنه (بهره‌برداری) یا یه مسیر جدید امتحان کنه که شاید کوتاه‌تر باشه (کاوش). پیدا کردن این تعادل خیلی مهمه، چون اگه زیادی کاوش کنه وقتو تلف می‌کنه و اگه فقط بهره‌برداری کنه شاید تو رفتارای غیربهینه گیر کنه.
            </p>
            
    
            <hr>
    
            <h2 id="rl-overview">Reinforcement Learning چیه؟ یه مسئله و یه راه‌حل</h2>
    
            <p>
                Reinforcement Learning هم به یه جور مسئله می‌گن و هم به یه سری روش‌های حل که برای حل اون مسئله به کار می‌رن:
            </p>
    
            <ul>
                <li><strong>یه مسئله:</strong> تو هسته خودش، RL مسئله یادگیری هدفمند تو محیطای نامطمئن رو بررسی می‌کنه. مثلاً یه ربات چطوری باید تصمیم بگیره که چه کاری انجام بده تا هم باتریشو حفظ کنه و هم کارشو تموم کنه؟ یا یه برنامه شطرنج چطور باید حرکتای خودشو انتخاب کنه که برنده بشه؟</li>
                <li><strong>یه راه‌حل:</strong> RL شامل یه سری روشای حل مثل Q-learning، policy gradients یا deep reinforcement learning هست که برای حل این مسئله طراحی شدن. این روشا به ایجنت کمک می‌کنن که از بازخورد یاد بگیره و رفته‌رفته بهتر بشه.</li>
            </ul>

            <p>همچنین میشه گفت که Reinforcement Learning به یه سری روش‌های حل گفته می‌شه که برای حل مسئله RL استفاده می‌شن.</p>

            <p>
                یه نکته خیلی مهم تو RL، تمایز بین مسائل و روشای حل هست. مثلاً، exploration exploitation dilemma یه مسئله مشترک تو خیلی از کارای RL هست، اما می‌شه با روشای مختلفی مثل استراتژی‌های ε-greedy یا Thompson sampling حلش کرد. اگه این تمایزو قاطی کنیم، خیلی چیزا اشتباه می‌شه، چون ممکنه یه مسئله بسته به شرایط راه‌حلای کاملاً متفاوتی بخواد.
            </p>

            <div class="box box-tip">
                <strong>کاوش بیشتر:</strong> استراتژی‌های ε-greedy رو یه نگاهی بنداز یا ببین "exploration" تو ویدیوگیم موردعلاقت چطوری انجام می‌شه. طراحای بازی چطوری این تعادلو برقرار می‌کنن؟
            </div>


    
            <hr>
    
            <h2 id="rl-vs-supervised">RL چه فرقی با Supervised و Unsupervised Learning داره؟</h2>
    
            <p>
                Reinforcement Learning معمولاً با بقیه پارادایم‌های یادگیری ماشین مثل <strong>supervised</strong> و <strong>unsupervised learning</strong> مقایسه می‌شه، ولی از اساس یه چیز کاملاً متفاوتیه.
            </p>
            <ul>
                <li><strong>Supervised Learning: </strong>تو یادگیری تحت نظارت (supervised)، ایجنت از یه مجموعه داده برچسب‌دار که یه سرپرست آماده کرده، یاد می‌گیره. مثلاً یه سیستم تشخیص گربه و سگ با کلی عکس که به عنوان "گربه" یا "سگ" برچسب خوردن آموزش می‌بینه. هدف اینه که ایجنت از این نمونه‌ها تعمیم بده و عکسای جدیدو درست دسته‌بندی کنه.

                    <div class="box box-question">
                        <strong>یه سؤال:</strong> اگه supervised شبیه یادگیری با یه معلم باشه، RL شبیه چیه؟ می‌شه گفت شبیه یادگیری از طریق کشف و تجربه است؟
                    </div>
                    ولی RL به داده‌های برچسب‌دار نیاز نداره. در عوض، مستقیماً از تعامل با محیط یاد می‌گیره. هیچ جواب مشخص و از پیش تعیین‌شده‌ای وجود نداره؛ ایجنت باید خودش کشفش کنه. مثلاً رباتی که یاد می‌گیره راه بره، دستورالعمل خاصی نمی‌گیره که چطوری پاهاشو حرکت بده—بلکه با امتحان کردن و دیدن نتیجه، خودش یاد می‌گیره.
                </li><br>
                <li><strong>Unsupervised Learning: </strong>تو یادگیری بدون نظارت (unsupervised)، هدف کشف الگوها یا ساختارهای پنهان تو داده‌هاست، مثل خوشه‌بندی مشتریا یا کاهش ابعاد داده‌ها. RL یه شباهتی به این روش داره چون به داده برچسب‌دار نیاز نداره، ولی تمرکزش بیشتر روی به حداکثر رسوندن سیگنال پاداش هست، نه کشف الگوها. مثلاً یه ایجنت RL که شطرنج بازی می‌کنه دنبال خوشه‌بندی حرکات شطرنج نیست—فقط می‌خواد بازیو ببره.
                    <div class="box box-question">
                        <strong>یه سؤال:</strong> چرا فکر می‌کنی RL بیشتر شبیه به آزمون و خطاست تا یادگیری تحت نظارت یا بدون نظارت؟ RL چه مسائلیو بهتر می‌تونه حل کنه؟
                    </div>
                    به طور کلی، RL یه موقعیت منحصربه‌فرد داره و به‌عنوان یه پارادایم سوم تو یادگیری ماشین شناخته می‌شه. RL کاوش، تعامل و رفتار هدف‌محوری رو ترکیب می‌کنه که تو بقیه پارادایما نیست، و برای حل مسائلی که راهنمایی مستقیم توشون وجود نداره، خیلی قویه.
                </li>
            </ul>
    
            
    
            <hr>
    
            <h2 id="rl-examples">مثال‌هایی از RL در عمل</h2>
    
            <ul>
                <li><strong>ماشین‌های خودران:</strong>این ماشینا از RL استفاده می‌کنن تا تو جاده حرکت کنن، از موانع رد شن و تصمیم‌گیریای لحظه‌ای کنن. ایجنت (ماشین) با محیط (شرایط جاده و ترافیک) تعامل داره و یاد می‌گیره ایمنی و کارایی رو به حداکثر برسونه و تصادفاتو کم کنه.
                    <div class="box box-tip">
                        <strong>یه مساله واقعی:</strong> تصور کن یه سیستم RL برای ماشینای خودران طراحی می‌کنی. چطوری به ماشین پاداش می‌دی؟ فقط اجتناب از تصادف کافیه یا برای مصرف انرژی کمتر یا راحتی مسافرا هم پاداش می‌ذاری؟
                    </div>
                </li>
                <li><strong>کنترل رباتیک:</strong> رباتایی که تو کارخونه قطعاتو مونتاژ می‌کنن، با استفاده از RL حرکات خودشونو بهبود می‌دن و دقتو بالا می‌برن و مصرف انرژی رو کم می‌کنن.</li>
                <li><strong>بازی‌ها:</strong> از AlphaGo که تو بازی Go استاد شد گرفته تا ایجنتای RL که تو بازیای پیچیده ویدیویی مهارت پیدا کردن، RL پتانسیلشو برای حل مسائل سخت با یادگیری مستقیم از تجربه نشون داده.</li>
            </ul>


            <hr>

            <h2 id="rl-elements">المنت های RL</h2>
            <p>
                Reinforcement Learning بدیهتاً چندتا المنت اصلی داره و این المنتا تعیین می‌کنن ایجنت چجوری یاد بگیره، خودش رو وفق بده و تو یه محیط رفتار کنه. این عناصر شامل Policy، Reward Signal، Value Function و به‌صورت اختیاری Model ای برای Environment هستن. هرکدوم از اینا یه نقش مشخص دارن که باعث می‌شن ایجنت درست کار کنه و بهتر یاد بگیره.
            </p>

            <div class="image-container">
                <img src="Pictures/2.png" alt="full scheme" style="width: 40%; height: auto;">
                <p class="image-caption">شکل 2: چرخه RL و چارچوب تعامل Agent و Environment<p>
            </div>


            <h3>Policy</h3>
            <p>
                Policy همون استراتژی یا راه‌وروشیه که ایجنت برای تصمیم‌گیری تو یه وضعیت خاص ازش استفاده می‌کنه. درواقع، Policy همون مغز ایجنته که مشخص می‌کنه تو یه شرایط خاص، چیکار کنه.
            </p>
            <p>مثلاً برای یه خودروی خودران:
                Policy تصمیم می‌گیره که ماشین باید به چپ بپیچه، گاز بده یا ترمز کنه، بسته به اطلاعاتی که از حسگرهاش می‌گیره (State). یه Policy ساده ممکنه بگه: «اگه جلوت مانع هست، ترمز کن». ولی یه Policy پیشرفته‌تر ممکنه از شبکه عصبی استفاده کنه تا ورودی‌های مختلف حسگرها رو تحلیل کنه و تصمیم دقیق‌تری بگیره.</p>
            <h4>ویژگی‌های Policy</h4>
            <ul>
                <li><strong>Deterministic یا Stochastic:</strong> یه Policy می‌تونه Deterministic باشه، یعنی یه وضعیت خاص همیشه به یه عمل خاص ختم می‌شه (مثلاً: «اگه تو وضعیت A باشی، همیشه عمل X رو انجام بده»). یا می‌تونه Stochastic باشه که توش به هر عمل یه احتمال داده می‌شه (مثلاً: «اگه تو وضعیت A باشی، عمل X رو با احتمال 70% و عمل Y رو با احتمال 30% انجام بده»).</li>
                <li><strong>قابلیت وفق پیدا کردن:</strong> Policy با گذر زمان بهتر می‌شه و سعی می‌کنه یاد بگیره کدوم کارها پاداش بیشتری دارن.</li>
                <li><strong>سادگی یا پیچیدگی:</strong> Policy می‌تونه خیلی ساده باشه، مثل یه جدول که وضعیت‌ها رو به عمل‌ها وصل می‌کنه، یا خیلی پیچیده باشه، مثل شبکه‌های عصبی که داده‌های پیچیده رو هندل می‌کنن.</li>
            </ul>
            <div class="box box-question">
                <strong>یه سؤال:</strong> اگه استراتژی درس خوندنت با نزدیک شدن به امتحان عوض بشه، فکر می‌کنی Policy تو Deterministic هست یا Stochastic؟
            </div>
            <div class="box box-tip">
                <strong>شهود:</strong> فرض کن داری شطرنج بازی می‌کنی. Policy تو، همون استراتژی ذهنیته که بر اساس موقعیت مهره‌ها روی صفحه (State)، تصمیم می‌گیره حرکت بعدی چیه. یه تازه‌کار ممکنه از Policy ساده‌ای مثل «همیشه شاه رو محافظت کن» استفاده کنه، ولی یه حرفه‌ای، Policy پیچیده‌تری داره که الگوها و حرکت‌های چند مرحله جلوتر رو در نظر می‌گیره.
            </div>
            <hr>
            
            <h3>Reward Signal</h3>
            <p>
                Reward Signal همون بازخوردیه که به ایجنت می‌گه کارش خوب بوده یا نه. میتونه یه عدد ساده‌ باشه که ایجنت بعد از هر کاری که می‌کنه از محیط می‌گیره و نشون‌دهنده موفقیت یا شکستشه.
            </p>
            <div class="box box-tip">
                <strong>توجه:</strong> پاداش می‌تونه پیچیده‌تر از یه عدد ساده باشه. معمولا پاداش‌ها به صورت یه مقدار اسکالر هستن که بازخورد مستقیم به یه اقدام رو نشون میدن، ولی بعضی وقتا لازمه که ساختار پاداش پیچیده‌تر باشه،
            </div>
            <h4>ویژگی‌های Reward Signal</h4>
            <ul>
                <li><strong>بازخورد فوری:</strong>پاداش‌ها تو هر لحظه داده می‌شن و نتیجه عمل اخیر رو نشون می‌دن.</li>
                <li><strong>تعریف هدف:</strong> پاداش‌ها مشخص می‌کنن هدف ایجنت چیه—یعنی اینکه مجموع پاداش‌ها رو تو زمان به حداکثر برسونه.</li>
            </ul>
            <div class="box box-question">
                <strong>یه سؤال:</strong> فرض کن داری به یه سگ آموزش می‌دی. اگه هر وقت سگ کاری که گفتی رو انجام داد بهش یه خوراکی بدی، اون خوراکی چه نقشی تو Reinforcement Learning داره؟ چجوری می‌تونی این پاداش‌ها رو طوری تنظیم کنی که ترفندهای پیچیده‌تری رو یاد بگیره؟
            </div>



            <hr>
            <h3>Value Function</h3>
            <p>
                Value Function نشون می‌ده چقدر خوبه که تو یه state خاص باشیم، با در نظر گرفتن پاداش‌های فوری و آینده. این تابع به ایجنت کمک می‌کنه روی کارهایی تمرکز کنه که شاید الان سودی نداشته باشن، ولی تو بلندمدت مفید باشن.
            </p>

            <h4>ویژگی‌های Value Function</h4>
            <ul>
                <li><strong>نگاه بلندمدت:</strong> Value پاداش‌های جمع‌شده تو طول زمان رو در نظر می‌گیره، نه فقط پاداش بعدی رو.</li>
                <li><strong>پویا بودن:</strong> Value‌ها با گذر زمان و تجربه ایجنت به‌روز می‌شن.</li>
                <li><strong>راهنمای بهتر شدن Policy:</strong> ایجنت از Value Function برای تصمیم‌گیری‌های بهتر استفاده می‌کنه.</li>
            </ul>

            <div class="box box-question">
                <strong>یه سؤال:</strong> تو زندگی، پیش میاد که منافع بلندمدت رو به رضایت کوتاه‌مدت ترجیح بدی؟ این شبیه نحوه استفاده ایجنت از Value Function نیست؟
            </div>

            <div class="box box-tip">
                <strong>شهود:</strong> فرض کن داری تو یه هزارتو راه می‌ری. حتی اگه یه مسیر مستقیم به خروجی نرسه (پاداش فوری)، ممکنه تو رو به نقطه‌ای نزدیک‌تر به خروجی برسونه (پاداش‌های آینده). ارزش بودن تو یه نقطه خاص از هزارتو به این بستگی داره که چقدر <strong>امیدوارکننده‌ست</strong> برای رسیدن به خروجی.
            </div>

            <hr>
            <h3>Model of the Environment (اختیاری)</h3>
            <p>
                Model پیش‌بینی می‌کنه که محیط چجوری به کارهای ایجنت واکنش نشون می‌ده. این به ایجنت اجازه می‌ده که وضعیت‌ها و پاداش‌های آینده رو شبیه‌سازی کنه و تصمیم‌گیری بهتری انجام بده.
            </p>
            <h4>ویژگی‌های Model</h4>
            <ul>
                <li><strong>توانایی پیش‌بینی:</strong> مدل می‌تونه حالت بعدی و پاداش رو بر اساس وضعیت فعلی و کاری که انجام شده تخمین بزنه.</li>
                <li><strong>کمک به برنامه‌ریزی:</strong> ایجنت با کمک مدل می‌تونه سناریوهای فرضی رو بررسی کنه و کارهایی رو انتخاب کنه که بیشترین پاداش رو تو آینده داشته باشن.</li>
                <li><strong>اختیاریه:</strong> همه روش‌های Reinforcement Learning از مدل استفاده نمی‌کنن. روش‌های Model-Free فقط با آزمون و خطا کار می‌کنن، ولی روش‌های Model-Based از برنامه‌ریزی استفاده می‌کنن.</li>
            </ul>

            <div class="box box-tip">
                <strong>شهود:</strong> فرض کن داری یه بازی تخته‌ای مثل شطرنج بازی می‌کنی. اگه بتونی حرکات احتمالی حریف رو پیش‌بینی کنی، داری از یه «مدل» استفاده می‌کنی. این پیش‌بینی بهت کمک می‌کنه استراتژی بهتری بچینی.
            </div>
            

            <div class="next-section">
                <a href="#next-section" class="button">بخش بعدی</a>
            </div>
    
        </section>
    </main>
    

    <footer class="social-footer">
        <a href="https://t.me/RL_Hub" target="_blank"><i class="fab fa-telegram"></i></a>
        <a href="https://github.com/The-RL-Hub" target="_blank"><i class="fab fa-github"></i></a>
        <a href="mailto:arshiyagharoony@gmail.com"><i class="fas fa-envelope"></i></a>
    </footer>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
    <script>
        hljs.highlightAll();
    </script>
    <script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
    
