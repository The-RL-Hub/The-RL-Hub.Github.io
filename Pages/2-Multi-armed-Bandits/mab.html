<!DOCTYPE html>
<html lang="fa" dir="rtl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RLH: Introduction</title>
    <link rel="stylesheet" href="styles.css">
    <script src="script.js" defer></script>
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Vazirmatn:wght@400;600&display=swap" rel="stylesheet">
    <!-- Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>
<body>
    <header>
        <div class="logo">
            <img src="Pictures/logo.png" alt="Logo">
            <span>مساله Multi-armed bandit </span>
        </div>
        <nav>
            <a href="https://the-rl-hub.github.io/">خانه</a>
            <a href="#">درباره ما</a>
            <a href="#"></a>
        </nav>
    </header>

    <main>
        <aside class="sidebar">
            <ul>
                <li><a href="#intro">مقدمه</a></li>
            </ul>
        </aside>
    
        <section class="content">
            <h1 id="intro">مقدمه‌ای بر Multi-Armed Bandits</h1>
            <p>
                مسئله‌ی Multi-Armed Bandit (MAB) یه مسئله‌ی مهم توی reinforcement learningـه که به چالش یادگیری از طریق آزمون و خطا می‌پردازه. این اسم از یه مقایسه با یه قمارباز تو کازینو گرفته شده که چند تا دستگاه اسلات (slot machine) جلوش داره. هر دستگاه یه احتمال نامعلوم برای جایزه دادن داره، و هدف اینه که با انتخاب درست، بیشترین سود رو توی طول زمان ببری.
            </p>
            <p>این مسئله کلی کاربرد داره، مثلاً:</p>
            <ul>
                <li><strong>تبلیغات آنلاین</strong> → انتخاب بهترین تبلیغ برای گرفتن بیشترین کلیک.</li>
                <li><strong>آزمایش‌های پزشکی</strong> → تست درمان‌های مختلف برای پیدا کردن بهترین روش درمانی.</li>
            </ul>

            
            <div class="image-container">
                <img src="Pictures/1.jpg" alt="A Robot" style="width: 50%; height: auto;">
                <p class="image-caption">شکل 1: تصویری از برگرفته از کورس CS188 دانشگاه برکلی<p>
            </div>
            
            
            <div class="box box-tip">
                همونطور که قبلاً هم گفته بودیم برخلاف supervised learning که توش مدل با داده‌های برچسب‌دار یاد می‌گیره، reinforcement learning از evaluative feedback استفاده می‌کنه، یعنی مدل بر اساس پاداش‌هایی که می‌گیره، خودش یاد می‌گیره که چی درسته، نه اینکه مستقیم بهش گفته بشه کدوم حرکت بهترینه.
            </div>

            <h2 id="nonassociative">یادگیری Nonassociative</h2>
            <p>
                مسئله‌ی MAB یه nonassociative learning حساب می‌شه، یعنی توش عامل نیازی نداره که برای هر وضعیت یه تصمیم جدا بگیره.
                هر بار که یه اقدام انتخاب می‌شه، مقدار پاداش فقط به اون اقدام بستگی داره، نه به وضعیت‌های مختلف محیط.
            </p>
            
            <div class="table-container">
                <table>
                    <tr>
                        <th>ویژگی</th>
                        <th>Nonassociative Learning (MAB)</th>
                        <th>Associative Learning (Full RL)</th>
                    </tr>
                    <tr>
                        <td>وابسته به وضعیت‌های مختلف؟</td>
                        <td>❌ نه</td>
                        <td>✅ آره</td>
                    </tr>
                    <tr>
                        <td>اقدامات همیشه یکی هستن؟</td>
                        <td>✅ آره</td>
                        <td>❌ نه، بسته به state فرق دارن</td>
                    </tr>
                    <tr>
                        <td>مثال کاربردی</td>
                        <td>انتخاب بین چند slot machine</td>
                        <td>رانندگی خودکار</td>
                    </tr>
                </table>
            </div>

            <div class="image-container">
                <img src="Pictures/2.png" alt="A Slot machine" style="width: 20%; height: auto;">
                <p class="image-caption">شکل 2: نمایشی از یک slot machine<p>
            </div>

            <div class="box box-tip">
                توی nonassociative learning، عامل نیاز نداره که تصمیماتش رو بر اساس یه وضعیت خاص بگیره، فقط باید بفهمه کدوم گزینه سود بیشتری می‌ده.
            </div>
            <div class="box box-tip">
                اما تو associative learning، تصمیمات بسته به شرایط تغییر می‌کنن. مثلاً تو شطرنج، بهترین حرکت بستگی به وضعیت صفحه داره.
            </div>

            
            <h2 id="problem-definition">تعریف مسئله‌ی Multi-Armed Bandit</h2>
            <p>
                تو k-armed bandit problem، یه عامل (agent) قراره بین k تا اقدام انتخاب کنه، که هر کدوم یه مقدار پاداش دارن که معلوم نیست چقدره. هدف اینه که با یادگیری از تجربه، بهترین اقدام رو پیدا کنه و بیشترین پاداش رو ببره.
            </p>

            <h3>تعریف ریاضی قضیه</h3>
            
            <ul>
                <li>عامل k تا action ممکن (بازوها/arms) داره که می‌تونه از بینشون انتخاب کنه.</li>
                <li>هر اقدام a یه پاداش عددی، 𝑅_𝑡، تولید می‌کنه که از یه توزیع احتمال ثابت ولی ناشناخته گرفته می‌شه.</li>
                <li>مقدار واقعی یه اقدام، میانگین مورد انتظار پاداششه:
                    <div class="math-block">
                        $$ q_*(a) = E[R_t | A_t = a] $$
                    </div>
                </li>
                <li>مقدار q_∗ (a) نامعلومه و عامل باید با استفاده از تجربه‌ی خودش، یه تخمین ازش بسازه، یعنی:
                    <div class="math-block">
                        $$ Q_t​(a)≈ q_*(a) $$
                    </div>
                </li>
            </ul>
            <div class="table-container">
                <table>
                    <tr>
                        <th>مفهوم</th>
                        <th>توضیح</th>
                    </tr>
                    <tr>
                        <td>k-armed Bandit</td>
                        <td>یه محیط که توش k تا گزینه برای انتخاب وجود داره.</td>
                    </tr>
                    <tr>
                        <td>پاداش \( R_t \)</td>
                        <td>امتیازی که بعد از انتخاب یه گزینه می‌گیری.</td>
                    </tr>
                    <tr>
                        <td>ارزش واقعی \( q_*(a) \)</td>
                        <td>مقدار واقعی و میانگین پاداش یه اقدام (ولی نامعلوم).</td>
                    </tr>
                    <tr>
                        <td>برآورد \( Q_t(a) \)</td>
                        <td>مقدار تخمینی که عامل از \( q_*(a) \) داره.</td>
                    </tr>
                </table>
            </div>
            <div class="box box-tip">
                اگه عامل مقدار واقعی q_∗(a) رو می‌دونست، همیشه می‌تونست بهترین اقدام رو انتخاب کنه. ولی چون این مقدار ناشناخته‌ست، باید از راه تجربه و آزمون و خطا یاد بگیره.
            </div>
            
            
            <h2 id="exploration-exploitation">Exploration vs. Exploitation</h2>
            <p>
                بزرگ‌ترین چالشی که تو MAB problems داریم، اینه که چطور بین این دو تا استراتژی تعادل برقرار کنیم:
            </p>
            <ul>
                <li><strong>Exploitation</strong> → انتخاب گزینه‌ای که تا حالا بیشترین پاداش رو داده.</li>
                <li><strong>Exploration</strong> → امتحان گزینه‌های جدید برای پیدا کردن احتمالی یه انتخاب بهتر.</li>
            </ul>
            
            <div class="table-container">
                <table>
                    <tr>
                        <th>استراتژی</th>
                        <th>چیکار می‌کنه؟</th>
                        <th>اثر کوتاه‌مدت</th>
                        <th>اثر بلندمدت</th>
                    </tr>
                    <tr>
                        <td>Exploitation</td>
                        <td>انتخاب گزینه‌ای که بیشترین مقدار پاداش رو تا الان داشته.</td>
                        <td>بیشترین پاداش فوری.</td>
                        <td>ممکنه گزینه‌های بهتر رو کشف نکنی.</td>
                    </tr>
                    <tr>
                        <td>Exploration</td>
                        <td>تست کردن گزینه‌هایی که کمتر انتخاب شدن.</td>
                        <td>ممکنه در لحظه پاداش کمتری بده.</td>
                        <td>اما توی بلندمدت، می‌تونه گزینه‌های بهتری پیدا کنه.</td>
                    </tr>
                </table>
            </div>
            <h3>یه مثال ساده</h3>
            <p>فرض کن رفتی یه رستوران جدید:</p>
            <ul>
                <li><strong>Exploitation:</strong> غذایی که قبلاً امتحان کردی و دوست داری رو دوباره سفارش می‌دی.</li>
                <li><strong>Exploration:</strong> یه غذای جدید رو امتحان می‌کنی، شاید خوشمزه‌تر باشه!</li>
            </ul>
            
            <h3>چالش اصلی: تعادل بین این دو</h3>
            <p>نمی‌تونی همزمان <strong>Exploit</strong> و <strong>Explore</strong> کنی، پس باید تصمیم بگیری:</p>
            <ul>
                <li><strong>کی باید explore کنی؟</strong> (برای کشف گزینه‌های بهتر)</li>
                <li><strong>کی باید exploit کنی؟</strong> (برای گرفتن بیشترین پاداش از گزینه‌ی خوب شناخته‌شده)</li>
            </ul>
            
            <div class="box box-tip">
                    اگه همیشه Exploit کنی، شاید هیچ‌وقت متوجه نشی که یه گزینه‌ی بهتر هم وجود داره.
                    ولی اگه زیادی Explore کنی، ممکنه وقت زیادی رو روی گزینه‌های ضعیف هدر بدی و خب هندل این دوتا یه مسالس.
            </div>

            <div class="next-section">
                <a href="https://the-rl-hub.github.io/Pages/1-Introduction/intro.html" class="button">بخش قبلی</a>
                <a href="#next-section" class="button">بخش بعدی</a>
            </div>
        </section>
    </main>
    

    <footer class="social-footer">
        <a href="https://t.me/RL_Hub" target="_blank"><i class="fab fa-telegram"></i></a>
        <a href="https://github.com/The-RL-Hub" target="_blank"><i class="fab fa-github"></i></a>
        <a href="mailto:arshiyagharoony@gmail.com"><i class="fas fa-envelope"></i></a>
    </footer>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
    <script>
        hljs.highlightAll();
    </script>
    <script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
    
